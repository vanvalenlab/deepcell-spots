import scipy
import numpy as np
import tensorflow as tf

from scipy.ndimage.morphology import distance_transform_edt
from tensorflow.python.keras.preprocessing.image import ImageDataGenerator
from keras_preprocessing.image.affine_transformations import transform_matrix_offset_center


def subpixel_distance_transform(point_list, image_shape, dy=1, dx=1):
    """
    for each pixel in image, return the vectorial distance to a point in point_list that is in the pixel nearest to it
    Args:
        point_list: (N,2) numpy array of point coordinates [y, x] (y before x as in image/matrix indexing)
        image_shape: (Ly,Lx) specifies the shape of an image that contains the coordinates - the coordinates should be in dy*[-0.5, Ly-0.5] x dx*[-0.5, Lx-0.5]
        dy: pixel width in y axis
        dx: pixel width in x axis

    Returns:
        nearest_point: (Ly, Lx) numpy array, nearest_point[i,j] is the index in point_list of a point
        in a point-containing pixel which is closest to pixel [i,j]. Note no uniqueness of the point or the pixel,
        since there could be several point-containing pixels with minimal distance to pixel [i,j] and there could be
        several points contained in the pixel [i,j] but only one is chosen
        delta_x[i,j], delta_y[i,j] are elements of the vectorial distance between the chosen point
        which nearest_point[i,j] refers to, and the center of the pixel [i,j] - which is at x=j*dx, y=i*dy
        delta_y: (Ly, Lx) numpy array of signed y distance between a point from point_list that is near pixel [i,j]
        and the center of the pixel
        delta_x: (Ly, Lx) numpy array of signed x distance between a point from point_list that is near pixel [i,j]
        and the center of the pixel
    """
    # create an image with 0 = pixel containing point from point_list, 1 = pixel not containing point from point_list
    contains_point = np.ones(image_shape) 
    nearest_point = np.full(image_shape, np.nan)  # index in point_list of point nearest to pixel

    # dictionary to be filled s.t.: pixel_to_contained_point_ind[(i,j)] = k if point_list[k]
    # is a point contained in pixel i,j of the image
    pixel_to_contained_point_ind = {}

    for ind, [y, x] in enumerate(point_list):
        nearest_pixel_y_ind = int(round(y/dy))
        nearest_pixel_x_ind = int(round(x/dx))
        contains_point[nearest_pixel_y_ind, nearest_pixel_x_ind] = 0
        pixel_to_contained_point_ind[(nearest_pixel_y_ind, nearest_pixel_x_ind)] = ind

    edt, inds = distance_transform_edt(contains_point, return_indices=True, sampling=[dy, dx])

    delta_y = np.full(image_shape, np.inf)  # signed y distance to nearest point
    delta_x = np.full(image_shape, np.inf)  # signed x distance to nearest point

    Ly, Lx = image_shape
    for j in range(0, Lx):
        for i in range(0, Ly):
            #inds[0][i,j] # y index of nearest pixel to [i,j] which contains a point
            #inds[1][i,j] # x index of nearest pixel to [i,j] which contains a point
            #nearest_point[i,j] # index in point_list of point contained by pixel [i,j]
            nearest_point[i, j] = pixel_to_contained_point_ind[(inds[0][i, j], inds[1][i, j])]
            delta_y[i, j] = dy * (point_list[int(nearest_point[i, j])][0] - i)
            delta_x[i, j] = dx*(point_list[int(nearest_point[i, j])][1] - j)
    
    return delta_y, delta_x, nearest_point


def generate_transformation_matrix(transform_parameters, image_shape, img_row_axis, img_col_axis):
    '''
    given a dictionary of affine transformation parameters (such as the one generated by the ImageDataGenerator function get_random_transform),
    generate the transformation matrix and offset which apply_affine_transform generates and passes to scipy.ndimage.interpolation.affine_transform:
    ndimage.interpolation.affine_transform(
            x_channel,
            final_affine_matrix,
            final_offset,
            order=order,
            mode=fill_mode,
            cval=cval)
    this function performs the calculations performed by tf.keras.preprocessing.image.apply_affine_transform
    to obtain final_affine_matrix and final_offset, and returns them.
    
    Args:
        transform_parameters: dictionary of affine transformation parameters such as 
        the output of ImageDataGenerator get_random_transform 
        (as used in input to apply_transform called on image)
            From keras-preprocessing/keras_preprocessing/image/image_data_generator.py, apply_transform documentation:
                Dictionary with string - parameter pairs describing the transformation.
                Currently, the following parameters
                from the dictionary are used:
                - `'theta'`: Float. Rotation angle in degrees.
                - `'tx'`: Float. Shift in the x direction.
                - `'ty'`: Float. Shift in the y direction.
                - `'shear'`: Float. Shear angle in degrees.
                - `'zx'`: Float. Zoom in the x direction.
                - `'zy'`: Float. Zoom in the y direction.
                - `'flip_horizontal'`: Boolean. Horizontal flip. - NOT USED HERE
                - `'flip_vertical'`: Boolean. Vertical flip. - NOT USED HERE
                - `'channel_shift_intensity'`: Float. Channel shift intensity.
                - `'brightness'`: Float. Brightness shift intensity.
        
    Returns:
        final_affine_matrix (2*2 matrix ,denote below: T)
        final_offset (length 2 vector, denote below: s)
        
    a point p in the output image of affine_transform corresponds to the point pT+s in the input image
    '''
    # get transform parameters from the input dictionary (if non given, the default value in apply_affine_transform is used)
    theta = transform_parameters.get('theta', 0)
    tx = transform_parameters.get('tx', 0)
    ty = transform_parameters.get('ty', 0)
    shear = transform_parameters.get('shear', 0)
    zx = transform_parameters.get('zx', 1)
    zy = transform_parameters.get('zy', 1)
    
    # generate the transform matrix and offset vector
    transform_matrix = None
    if theta != 0:
        theta = np.deg2rad(theta)
        rotation_matrix = np.array([[np.cos(theta), -np.sin(theta), 0],
                                    [np.sin(theta), np.cos(theta), 0],
                                    [0, 0, 1]])
        transform_matrix = rotation_matrix
    
    if tx != 0 or ty != 0:
        shift_matrix = np.array([[1, 0, tx],
                                 [0, 1, ty],
                                 [0, 0, 1]])
        if transform_matrix is None:
            transform_matrix = shift_matrix
        else:
            transform_matrix = np.dot(transform_matrix, shift_matrix)
    
    if shear != 0:
        shear = np.deg2rad(shear)
        shear_matrix = np.array([[1, -np.sin(shear), 0],
                                 [0, np.cos(shear), 0],
                                 [0, 0, 1]])
        if transform_matrix is None:
            transform_matrix = shear_matrix
        else:
            transform_matrix = np.dot(transform_matrix, shear_matrix)
    
    if zx != 1 or zy != 1:
        zoom_matrix = np.array([[zx, 0, 0],
                                [0, zy, 0],
                                [0, 0, 1]])
        if transform_matrix is None:
            transform_matrix = zoom_matrix
        else:
            transform_matrix = np.dot(transform_matrix, zoom_matrix)
    
    
    if transform_matrix is None:  # if no shift, shear or zoom are done, the transformation is the identity
        transform_matrix = np.array([[1, 0, 0],
                                 [0, 1, 0],
                                 [0, 0, 1]])
    # if transform_matrix is not None:
    h, w = image_shape[img_row_axis], image_shape[img_col_axis]
    transform_matrix = transform_matrix_offset_center(transform_matrix, h, w)
    final_affine_matrix = transform_matrix[:2, :2]
    final_offset = transform_matrix[:2, 2]

    return final_affine_matrix, final_offset


def affine_transform_points(points, transform_parameters,
    image_shape, img_row_axis=0, img_col_axis=1, fill_mode='nearest'):
    '''
    Perform an affine transform mapping input coordinates referring to the input image of the apply_transform function of the class ImageDataGenerator 
    To the output space of that function. Returned points are (original and padding points) contained in the output image.
    
    Args:
        transform_parameters: dictionary of affine transformation parameters such as 
            the output of ImageDataGenerator get_random_transform 
            
        points: (N, 2) numpy array which contains points in the format [y, x]
            (NOTE: as in image/matrix notation, not Cartesian notation)
            points are labels for the input image - they should be -0.5 <= x <= Lx-0.5, -0.5 <= y <= Ly-0.5
            where Lx = image_shape[img_col_axis], Ly = image_shape[img_row_axis]

        image_shape (tuple): the shape of the image which contains the points (for 2D image, has length 2)

        img_row_axis: the index of the axis (0 or 1) to be flipped when flip_vertical is True

        img_col_axis: the index of the axis (0 or 1) to be flipped when flip_horizontal is True 

        fill_mode: One of {"constant", "nearest", "reflect" or "wrap"}.
            Default is 'nearest'. Points outside the boundaries of the input
            are filled according to the given mode:
                'constant': kkkkkkkk|abcd|kkkkkkkk (cval=k)
                'nearest':  aaaaaaaa|abcd|dddddddd
                'reflect':  abcddcba|abcd|dcbaabcd
                'wrap':     abcdabcd|abcd|abcdabcd
    
    Returns:
        transformed_points: list of points / or numpy array of shape (N',2) which contains points in the format [y, x]
            Note N' != N because points in the original image may fall outside of the transformed output image.
            Also, if fill_mode is 'reflect' or 'wrap', point images in the padding of the input image can be inside the output image.
    '''
    
    transform, offset = generate_transformation_matrix(transform_parameters, image_shape, img_row_axis, img_col_axis)
    
    # add padding point image labels to points array
    if fill_mode=='reflect' or fill_mode=='wrap':
        # shorten notation of image size
        Ly = image_shape[img_row_axis]
        Lx = image_shape[img_col_axis]
        # create point images in the unit cells overlapping the parallelogram domain in the input space which is
        # transformed into the output image range
        corner_points = np.array([[0, 0], [0, Lx], [Ly, 0], [Ly, Lx]]) - 0.5 # coordinates of the corner points of the image
        # apply the output image -> input image space affine transform on the corner points, to get the cornet points
        # of the parallelogram domain in input space which is mapped to the output image
        input_parallelogram_corners = np.dot(transform, corner_points.T).T + offset
        # the input space is tiled by rectangles (unit cells) = the input image. Index them:
        # (0,0) are the y and x indices of the input image itself
        # i = ..., -2, -1, 0, 1, 2, ... are y indices for cells above and below the y position of the input image
        # j = ..., -2, -1, 0, 1, 2, ... are x indices for cells to the left and right of the input image x position
        # find the unit cells that potentially intersect with the parallelogram domain that is mapped to the output:
        y_cell_inds = input_parallelogram_corners[:, 0]//Ly
        x_cell_inds = input_parallelogram_corners[:, 1]//Lx
        y_cell_min_ind = np.min(y_cell_inds)
        y_cell_max_ind = np.max(y_cell_inds)
        x_cell_min_ind = np.min(x_cell_inds)
        x_cell_max_ind = np.max(x_cell_inds)
        # list indices of unit cells that may be transformed into the output image range
        y_cell_ind_list = range(int(y_cell_min_ind), int(y_cell_max_ind+1))
        x_cell_ind_list = range(int(x_cell_min_ind), int(x_cell_max_ind+1))
        
        point_images = np.empty((0, 2))  # allocate empty np.array to append point images from each unit cell

        if fill_mode == 'reflect':
            for i in y_cell_ind_list:
                for j in x_cell_ind_list:
                    this_cell_points = points.copy()
                    this_cell_points[:, 0] = (i*Ly + this_cell_points[:, 0]) if (i % 2 == 0) else ((i+1)*Ly - this_cell_points[:, 0])
                    this_cell_points[:, 1] = (j*Lx + this_cell_points[:, 1]) if (j % 2 == 0) else ((j+1)*Lx - this_cell_points[:, 1])
                    point_images = np.append(point_images, this_cell_points, axis=0)
        elif fill_mode == 'wrap':
            for i in y_cell_ind_list:
                for j in x_cell_ind_list:
                    this_cell_points = points.copy()
                    this_cell_points[:, 0] = this_cell_points[:, 0] + i*Ly
                    this_cell_points[:, 1] = this_cell_points[:, 1] + j*Lx
                    point_images = np.append(point_images, this_cell_points, axis=0)
      
        transformed_points = np.dot(np.linalg.inv(transform), (np.array(point_images) - offset).T)
        
    else:  # no point images added, transform just the input points
        transformed_points = np.dot(np.linalg.inv(transform), (np.array(points) - offset).T)
    
    # apply horizontal and vertical flip (if needed)
    def flip_axis_point(x, image_shape_axis):
        '''flip coordinate x around center of (-0.5, image_shape_axis-0.5)
        This gives the coordinate matching x in a flipped image with shape[axis] == image_shape_axis
        '''
        return image_shape_axis - 1 - x

    transformed_points = transformed_points.T  # transpose points back to point per row format
    
    if transform_parameters.get('flip_horizontal', False):
        transformed_points[:, img_col_axis] = flip_axis_point(transformed_points[:, img_col_axis], image_shape[img_col_axis])
    
    if transform_parameters.get('flip_vertical', False):
        transformed_points[:, img_row_axis] = flip_axis_point(transformed_points[:, img_row_axis], image_shape[img_row_axis])
    
    # delete transformed points that are not inside the output image
    def point_in_image(points, image_shape, img_row_axis, img_col_axis):
        p_in_y = (-0.5 <= points[:, img_row_axis]) & (points[:, img_row_axis] <= (image_shape[img_row_axis] - 0.5))
        p_in_x = (-0.5 <= points[:, img_col_axis]) & (points[:, img_col_axis] <= (image_shape[img_col_axis] - 0.5))
        res = p_in_y & p_in_x
        return res
    
    transformed_points_in_image = transformed_points[point_in_image(transformed_points, image_shape, img_row_axis, img_col_axis),:]
    
    return transformed_points_in_image


def transform_matrix_offset_center(matrix, x, y):
    o_x = float(x) / 2 + 0.5
    o_y = float(y) / 2 + 0.5
    offset_matrix = np.array([[1, 0, o_x], [0, 1, o_y], [0, 0, 1]], dtype='float32')
    reset_matrix = np.array([[1, 0, -o_x], [0, 1, -o_y], [0, 0, 1]], dtype='float32')

    offset_matrix = tf.convert_to_tensor(offset_matrix)
    reset_matrix = tf.convert_to_tensor(reset_matrix)

    transform_matrix = tf.keras.backend.dot(tf.keras.backend.dot(offset_matrix, matrix), reset_matrix)
    return transform_matrix


def compute_random_transform_matrix(augmentation_kwargs, img_height, img_width):
    rotation_range = augmentation_kwargs['rotation_range']
    zoom_range = augmentation_kwargs['zoom_range']
    horizontal_flip = augmentation_kwargs['horizontal_flip']
    vertical_flip = augmentation_kwargs['vertical_flip']

    # Get random angles
    theta = tf.random.uniform(shape=(1,), 
                              minval=-np.pi*rotation_range/180, 
                              maxval=np.pi*rotation_range/180)
    one = tf.constant(1.0, shape=(1,))
    zero = tf.constant(0.0, shape=(1,))
    cos_theta = tf.math.cos(theta)
    sin_theta = tf.math.sin(theta)

    rot_row_0 = tf.stack([cos_theta, -sin_theta, zero], axis=1)
    rot_row_1 = tf.stack([sin_theta, cos_theta, zero], axis=1)
    rot_row_2 = tf.stack([zero, zero, one], axis=1)
    rotation_matrix = tf.concat([rot_row_0, rot_row_1, rot_row_2], axis=0)

    transform_matrix = rotation_matrix

    # # Get random lr flips
    # lr = 2*tf.cast(tf.random.categorical(tf.math.log([[0.5, 0.5]]), 1), 'float32')[0] - 1.0
    # lr_row_0 = tf.stack([lr, zero, zero], axis=1)
    # lr_row_1 = tf.stack([zero, one, zero], axis=1)
    # lr_row_2 = tf.stack([zero, zero, one], axis=1)
    # lr_flip_matrix = tf.concat([lr_row_0, lr_row_1, lr_row_2], axis=0)

    # transform_matrix = tf.keras.backend.dot(transform_matrix, lr_flip_matrix)

    # # Get randum ud flips
    # ud = 2*tf.cast(tf.random.categorical(tf.math.log([[0.5, 0.5]]), 1), 'float32')[0] - 1.0
    # ud_row_0 = tf.stack([one, zero, zero], axis=1)
    # ud_row_1 = tf.stack([zero, ud, zero], axis=1)
    # ud_row_2 = tf.stack([zero, zero, one], axis=1)
    # ud_flip_matrix = tf.concat([ud_row_0, ud_row_1, ud_row_2], axis=0)

    # transform_matrix = tf.keras.backend.dot(transform_matrix, ud_flip_matrix)

    # Get random zooms
    zx = tf.random.uniform(shape=(1,), minval=zoom_range[0], maxval=zoom_range[1])
    zy = tf.random.uniform(shape=(1,), minval=zoom_range[0], maxval=zoom_range[1])
    z_row_0 = tf.stack([zx, zero, zero], axis=1)
    z_row_1 = tf.stack([zero, zy, zero], axis=1)
    z_row_2 = tf.stack([zero, zero, one], axis=1)
    zoom_matrix = tf.concat([z_row_0, z_row_1, z_row_2], axis=0)

    transform_matrix = tf.keras.backend.dot(transform_matrix, zoom_matrix)

    # Combine all matrices
    h, w = img_height, img_width
    transform_matrix = transform_matrix_offset_center(transform_matrix, h, w)

    return transform_matrix       